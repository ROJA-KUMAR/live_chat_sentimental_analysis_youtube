# -*- coding: utf-8 -*-
"""dl

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egwrtHbCt5o_OCz-NHEtkcIscGogctFf
"""

from textblob import TextBlob
import csv
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Replace 'YOUR_API_KEY' with your actual API key
api_key = 'AIzaSyDqbUzHXLkcVVmgUxwcbubYT_uK5CBP7mg'

# Create a YouTube Data API service
youtube = build('youtube', 'v3', developerKey=api_key)

# Example: Get the live chat ID of a live video (replace 'VIDEO_ID' with the actual video ID)
video_id = 'kB8CSmLxAgk'
# Set a larger 'maxResults' value to retrieve more chat messages per request
max_results = 1000

# Function to classify emotions
def classify_emotion(text):
    analysis = TextBlob(text)
    sentiment = analysis.sentiment

    if sentiment.polarity > 0:
        return "postive"
    else:
        return "negative"
    

# Create a list to store chat messages and their sentiments
chat_sentiments = []

try:
    video_response = youtube.videos().list(
        part='liveStreamingDetails',
        id=video_id
    ).execute()

    live_chat_id = video_response['items'][0]['liveStreamingDetails']['activeLiveChatId']

    # Continuously retrieve and process live chat messages
    while True:
        live_chat_response = youtube.liveChatMessages().list(
            liveChatId=live_chat_id,
            part='snippet',
            maxResults=max_results
        ).execute()

        for message in live_chat_response.get('items', []):
            snippet = message.get('snippet', {})
            author = snippet.get('authorDisplayName', 'System Message')
            message_text = snippet.get('displayMessage', '')
            published_at = snippet.get('publishedAt', '')

            # Perform sentiment analysis using the classify_emotion function
            emotion = classify_emotion(message_text)

            chat_sentiments.append([author, message_text, emotion, published_at])

        # Continue retrieving more messages by using the nextPageToken
        nextPageToken = live_chat_response.get('nextPageToken')
        if nextPageToken:
            live_chat_response = youtube.liveChatMessages().list(
                liveChatId=live_chat_id,
                part='snippet',
                maxResults=max_results,
                pageToken=nextPageToken
            ).execute()
        else:
            # Add your custom logic here for real-time processing
            pass  # This is a placeholder; add your logic here

except HttpError as e:
    print(f"An error occurred: {e}")

# Write chat sentiments to a CSV file
csv_file = 'live_chat_sentiments5.csv'

with open(csv_file, 'w', newline='', encoding='utf-8') as file:
    csv_writer = csv.writer(file)
    csv_writer.writerow(['Author', 'Message', 'Emotion', 'Published At'])  # Write header row
    csv_writer.writerows(chat_sentiments)


print(f"Chat sentiments have been written to {csv_file}")

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.preprocessing import LabelEncoder

# Load the data into a DataFrame
data = pd.read_csv('live_chat_sentiments5.csv')

# Preprocess the text data
data['Message'] = data['Message'].str.lower()  # Convert text to lowercase
data['Message'] = data['Message'].str.replace('[^\w\s]', '')  # Remove punctuation

# Encode emotions
label_encoder = LabelEncoder()
data['Emotion'] = label_encoder.fit_transform(data['Emotion'])

# Tokenize text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['Message'])
X = tokenizer.texts_to_sequences(data['Message'])
X = pad_sequences(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, data['Emotion'], test_size=0.2, random_state=42)

# Build an LSTM model
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=X.shape[1]))
model.add(LSTM(128))
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))


# Save label encoder classes to a CSV file
label_encoder_classes = pd.Series(label_encoder.classes_)
label_encoder_classes.to_csv('label_encoder_classes.csv', index=False)

# Evaluate the model on the testing data
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Model Accuracy: {accuracy}")

# Live Sentiment Analysis
def perform_live_sentiment_analysis(model, tokenizer, text):
    # Preprocess the incoming text message
    text = text.lower()
    text = text.replace('[^\w\s]', '')

    # Tokenize and pad the text
    sequence = tokenizer.texts_to_sequences([text])
    sequence = pad_sequences(sequence, maxlen=X.shape[1])

    # Predict the emotion
    prediction = model.predict(sequence)
    predicted_emotion = label_encoder.inverse_transform([prediction.argmax()])[0]

    return predicted_emotion

# Example usage for live sentiment analysis
new_message = input("Enter a txt message")
predicted_emotion = perform_live_sentiment_analysis(model, tokenizer, new_message)
print(f"Predicted Emotion: {predicted_emotion}")

model.save("dl.h5")